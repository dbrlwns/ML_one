{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GglF1G2IbRRo"},"outputs":[],"source":["# cnn\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets\n","\n","USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","\n","# Hyper parameters\n","EPOCHS = 40\n","BATCH_SIZE = 64\n","\n","# dataset load\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.FashionMNIST('./.data',\n","                          train=True,\n","                          download=True,\n","                          transform=transforms.Compose([\n","                              transforms.ToTensor(),\n","                              transforms.Normalize((0.1307, ), (0.3081, ))\n","                          ])),\n","    batch_size=BATCH_SIZE, shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.FashionMNIST('./.data',\n","                          train=False,\n","                          download=True,\n","                          transform=transforms.Compose([\n","                              transforms.ToTensor(),\n","                              transforms.Normalize((0.1307, ), (0.3081, ))\n","                          ])),\n","    batch_size=BATCH_SIZE, shuffle=True)\n","\n","\n","# Convolution Net\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","\n","        self.drop = nn.Dropout2d()\n","\n","        self.fc1 = nn.Linear(320, 50)\n","        self.fc2 = nn.Linear(50, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n","\n","        x = x.view(-1, 320)\n","\n","        x = F.relu(self.fc1(x))\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)\n","\n","\n","model = CNN().to(DEVICE)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n","\n","def train(model, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(DEVICE), target.to(DEVICE)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.cross_entropy(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if(batch_idx % 200 == 0):\n","            print('Train Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.\n","                  format(epoch, batch_idx * len(data),\n","                         len(train_loader.dataset),\n","                         100. * batch_idx / len(train_loader),\n","                         loss.item()))\n","\n","\n","def evaluate(model, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(DEVICE), target.to(DEVICE)\n","            output = model(data)\n","\n","            # 배치 오차 합산\n","            test_loss += F.cross_entropy(output, target,\n","                                    reduction='sum').item()\n","\n","            # 가장 큰 값의 인덱스가 예측값임\n","            pred = output.max(1, keepdim=True)[1]\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","        test_loss /= len(test_loader.dataset)\n","        test_accuracy = 100. * correct / len(test_loader.dataset)\n","        return test_loss, test_accuracy\n","\n","\n","\n","# run\n","for epoch in range(1, EPOCHS + 1):\n","    train(model, train_loader, optimizer, epoch)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","\n","    print(\"[{}] Test Loss : {:.4f}, Accuracy : {:.2f}%\".format(\n","        epoch, test_loss, test_accuracy\n","    ))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"BS0YOAd3yJ8Y"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Train Epoch : 1 [0/50000 (0%)]\tLoss: 2.374475\n","Train Epoch : 1 [25600/50000 (51%)]\tLoss: 1.568350\n","[1] Test Loss : 1.3790, Accuracy : 50.24%\n","Train Epoch : 2 [0/50000 (0%)]\tLoss: 1.288828\n","Train Epoch : 2 [25600/50000 (51%)]\tLoss: 1.189685\n","[2] Test Loss : 1.2402, Accuracy : 56.44%\n","Train Epoch : 3 [0/50000 (0%)]\tLoss: 0.984990\n","Train Epoch : 3 [25600/50000 (51%)]\tLoss: 0.854809\n","[3] Test Loss : 1.0919, Accuracy : 63.89%\n","Train Epoch : 4 [0/50000 (0%)]\tLoss: 0.896236\n","Train Epoch : 4 [25600/50000 (51%)]\tLoss: 0.740307\n","[4] Test Loss : 1.1433, Accuracy : 62.22%\n","Train Epoch : 5 [0/50000 (0%)]\tLoss: 0.688749\n","Train Epoch : 5 [25600/50000 (51%)]\tLoss: 0.709998\n","[5] Test Loss : 1.0837, Accuracy : 66.46%\n","Train Epoch : 6 [0/50000 (0%)]\tLoss: 0.754176\n","Train Epoch : 6 [25600/50000 (51%)]\tLoss: 0.609063\n","[6] Test Loss : 0.7218, Accuracy : 76.18%\n","Train Epoch : 7 [0/50000 (0%)]\tLoss: 0.793529\n","Train Epoch : 7 [25600/50000 (51%)]\tLoss: 0.643364\n","[7] Test Loss : 0.7416, Accuracy : 74.77%\n","Train Epoch : 8 [0/50000 (0%)]\tLoss: 0.546352\n","Train Epoch : 8 [25600/50000 (51%)]\tLoss: 0.495982\n","[8] Test Loss : 0.7694, Accuracy : 74.45%\n","Train Epoch : 9 [0/50000 (0%)]\tLoss: 0.492491\n","Train Epoch : 9 [25600/50000 (51%)]\tLoss: 0.509727\n","[9] Test Loss : 1.0493, Accuracy : 67.40%\n","Train Epoch : 10 [0/50000 (0%)]\tLoss: 0.668177\n","Train Epoch : 10 [25600/50000 (51%)]\tLoss: 0.586683\n","[10] Test Loss : 0.6622, Accuracy : 77.92%\n","Train Epoch : 11 [0/50000 (0%)]\tLoss: 0.533264\n","Train Epoch : 11 [25600/50000 (51%)]\tLoss: 0.730043\n","[11] Test Loss : 0.8427, Accuracy : 73.38%\n","Train Epoch : 12 [0/50000 (0%)]\tLoss: 0.559748\n","Train Epoch : 12 [25600/50000 (51%)]\tLoss: 0.482055\n","[12] Test Loss : 0.7135, Accuracy : 76.43%\n","Train Epoch : 13 [0/50000 (0%)]\tLoss: 0.523770\n","Train Epoch : 13 [25600/50000 (51%)]\tLoss: 0.488585\n","[13] Test Loss : 0.6922, Accuracy : 76.84%\n","Train Epoch : 14 [0/50000 (0%)]\tLoss: 0.567775\n","Train Epoch : 14 [25600/50000 (51%)]\tLoss: 0.528740\n","[14] Test Loss : 0.6799, Accuracy : 77.82%\n","Train Epoch : 15 [0/50000 (0%)]\tLoss: 0.609067\n","Train Epoch : 15 [25600/50000 (51%)]\tLoss: 0.633121\n","[15] Test Loss : 0.8790, Accuracy : 72.09%\n","Train Epoch : 16 [0/50000 (0%)]\tLoss: 0.776679\n","Train Epoch : 16 [25600/50000 (51%)]\tLoss: 0.570699\n","[16] Test Loss : 0.6524, Accuracy : 77.93%\n","Train Epoch : 17 [0/50000 (0%)]\tLoss: 0.696767\n","Train Epoch : 17 [25600/50000 (51%)]\tLoss: 0.403310\n","[17] Test Loss : 0.8624, Accuracy : 73.46%\n","Train Epoch : 18 [0/50000 (0%)]\tLoss: 0.409065\n","Train Epoch : 18 [25600/50000 (51%)]\tLoss: 0.612865\n","[18] Test Loss : 0.5849, Accuracy : 80.20%\n","Train Epoch : 19 [0/50000 (0%)]\tLoss: 0.468747\n","Train Epoch : 19 [25600/50000 (51%)]\tLoss: 0.450841\n","[19] Test Loss : 0.6369, Accuracy : 78.75%\n","Train Epoch : 20 [0/50000 (0%)]\tLoss: 0.617557\n","Train Epoch : 20 [25600/50000 (51%)]\tLoss: 0.498950\n","[20] Test Loss : 0.9016, Accuracy : 72.02%\n","Train Epoch : 21 [0/50000 (0%)]\tLoss: 0.631275\n","Train Epoch : 21 [25600/50000 (51%)]\tLoss: 0.529199\n","[21] Test Loss : 0.7937, Accuracy : 74.97%\n","Train Epoch : 22 [0/50000 (0%)]\tLoss: 0.359843\n","Train Epoch : 22 [25600/50000 (51%)]\tLoss: 0.386809\n","[22] Test Loss : 0.6543, Accuracy : 78.12%\n","Train Epoch : 23 [0/50000 (0%)]\tLoss: 0.530591\n","Train Epoch : 23 [25600/50000 (51%)]\tLoss: 0.511566\n","[23] Test Loss : 0.9982, Accuracy : 70.04%\n","Train Epoch : 24 [0/50000 (0%)]\tLoss: 0.457030\n","Train Epoch : 24 [25600/50000 (51%)]\tLoss: 0.574861\n","[24] Test Loss : 0.6848, Accuracy : 77.62%\n","Train Epoch : 25 [0/50000 (0%)]\tLoss: 0.480154\n","Train Epoch : 25 [25600/50000 (51%)]\tLoss: 0.485101\n","[25] Test Loss : 0.7960, Accuracy : 74.45%\n","Train Epoch : 26 [0/50000 (0%)]\tLoss: 0.581510\n","Train Epoch : 26 [25600/50000 (51%)]\tLoss: 0.614173\n","[26] Test Loss : 0.6927, Accuracy : 77.43%\n","Train Epoch : 27 [0/50000 (0%)]\tLoss: 0.534378\n","Train Epoch : 27 [25600/50000 (51%)]\tLoss: 0.476745\n","[27] Test Loss : 0.7109, Accuracy : 77.04%\n","Train Epoch : 28 [0/50000 (0%)]\tLoss: 0.529392\n","Train Epoch : 28 [25600/50000 (51%)]\tLoss: 0.448126\n","[28] Test Loss : 0.6974, Accuracy : 77.81%\n","Train Epoch : 29 [0/50000 (0%)]\tLoss: 0.494531\n","Train Epoch : 29 [25600/50000 (51%)]\tLoss: 0.430095\n","[29] Test Loss : 0.6217, Accuracy : 79.01%\n","Train Epoch : 30 [0/50000 (0%)]\tLoss: 0.537871\n","Train Epoch : 30 [25600/50000 (51%)]\tLoss: 0.538299\n","[30] Test Loss : 1.0390, Accuracy : 70.78%\n","Train Epoch : 31 [0/50000 (0%)]\tLoss: 0.522803\n","Train Epoch : 31 [25600/50000 (51%)]\tLoss: 0.585699\n","[31] Test Loss : 0.8090, Accuracy : 75.76%\n","Train Epoch : 32 [0/50000 (0%)]\tLoss: 0.456609\n","Train Epoch : 32 [25600/50000 (51%)]\tLoss: 0.614382\n","[32] Test Loss : 0.7595, Accuracy : 75.76%\n","Train Epoch : 33 [0/50000 (0%)]\tLoss: 0.486387\n","Train Epoch : 33 [25600/50000 (51%)]\tLoss: 0.571221\n","[33] Test Loss : 0.8894, Accuracy : 74.02%\n","Train Epoch : 34 [0/50000 (0%)]\tLoss: 0.499377\n","Train Epoch : 34 [25600/50000 (51%)]\tLoss: 0.416073\n","[34] Test Loss : 0.7161, Accuracy : 76.51%\n","Train Epoch : 35 [0/50000 (0%)]\tLoss: 0.401429\n","Train Epoch : 35 [25600/50000 (51%)]\tLoss: 0.487740\n","[35] Test Loss : 0.8207, Accuracy : 75.56%\n","Train Epoch : 36 [0/50000 (0%)]\tLoss: 0.583484\n","Train Epoch : 36 [25600/50000 (51%)]\tLoss: 0.484733\n","[36] Test Loss : 0.6629, Accuracy : 77.90%\n","Train Epoch : 37 [0/50000 (0%)]\tLoss: 0.483873\n","Train Epoch : 37 [25600/50000 (51%)]\tLoss: 0.399404\n","[37] Test Loss : 0.6824, Accuracy : 78.17%\n","Train Epoch : 38 [0/50000 (0%)]\tLoss: 0.633621\n","Train Epoch : 38 [25600/50000 (51%)]\tLoss: 0.499218\n","[38] Test Loss : 0.6257, Accuracy : 78.71%\n","Train Epoch : 39 [0/50000 (0%)]\tLoss: 0.444919\n","Train Epoch : 39 [25600/50000 (51%)]\tLoss: 0.466546\n","[39] Test Loss : 0.6374, Accuracy : 78.63%\n","Train Epoch : 40 [0/50000 (0%)]\tLoss: 0.489035\n","Train Epoch : 40 [25600/50000 (51%)]\tLoss: 0.406774\n","[40] Test Loss : 0.7915, Accuracy : 74.23%\n","Train Epoch : 41 [0/50000 (0%)]\tLoss: 0.460030\n","Train Epoch : 41 [25600/50000 (51%)]\tLoss: 0.571537\n","[41] Test Loss : 0.5770, Accuracy : 80.66%\n","Train Epoch : 42 [0/50000 (0%)]\tLoss: 0.414980\n","Train Epoch : 42 [25600/50000 (51%)]\tLoss: 0.369074\n","[42] Test Loss : 0.7016, Accuracy : 77.20%\n","Train Epoch : 43 [0/50000 (0%)]\tLoss: 0.498039\n","Train Epoch : 43 [25600/50000 (51%)]\tLoss: 0.385381\n","[43] Test Loss : 1.1788, Accuracy : 65.18%\n","Train Epoch : 44 [0/50000 (0%)]\tLoss: 0.471135\n","Train Epoch : 44 [25600/50000 (51%)]\tLoss: 0.452674\n","[44] Test Loss : 0.6677, Accuracy : 78.67%\n","Train Epoch : 45 [0/50000 (0%)]\tLoss: 0.427772\n","Train Epoch : 45 [25600/50000 (51%)]\tLoss: 0.442120\n","[45] Test Loss : 0.8238, Accuracy : 74.81%\n","Train Epoch : 46 [0/50000 (0%)]\tLoss: 0.303443\n","Train Epoch : 46 [25600/50000 (51%)]\tLoss: 0.549569\n","[46] Test Loss : 0.6088, Accuracy : 79.54%\n","Train Epoch : 47 [0/50000 (0%)]\tLoss: 0.433309\n","Train Epoch : 47 [25600/50000 (51%)]\tLoss: 0.494432\n","[47] Test Loss : 0.7235, Accuracy : 76.98%\n","Train Epoch : 48 [0/50000 (0%)]\tLoss: 0.421100\n","Train Epoch : 48 [25600/50000 (51%)]\tLoss: 0.585902\n","[48] Test Loss : 0.6630, Accuracy : 78.76%\n","Train Epoch : 49 [0/50000 (0%)]\tLoss: 0.705017\n","Train Epoch : 49 [25600/50000 (51%)]\tLoss: 0.482500\n","[49] Test Loss : 0.6103, Accuracy : 79.58%\n","Train Epoch : 50 [0/50000 (0%)]\tLoss: 0.416784\n","Train Epoch : 50 [25600/50000 (51%)]\tLoss: 0.337047\n","[50] Test Loss : 0.3492, Accuracy : 88.26%\n","Train Epoch : 51 [0/50000 (0%)]\tLoss: 0.193815\n","Train Epoch : 51 [25600/50000 (51%)]\tLoss: 0.278948\n","[51] Test Loss : 0.3349, Accuracy : 88.76%\n","Train Epoch : 52 [0/50000 (0%)]\tLoss: 0.261272\n","Train Epoch : 52 [25600/50000 (51%)]\tLoss: 0.215978\n","[52] Test Loss : 0.3273, Accuracy : 88.97%\n","Train Epoch : 53 [0/50000 (0%)]\tLoss: 0.200805\n","Train Epoch : 53 [25600/50000 (51%)]\tLoss: 0.380014\n","[53] Test Loss : 0.3254, Accuracy : 89.13%\n","Train Epoch : 54 [0/50000 (0%)]\tLoss: 0.172486\n","Train Epoch : 54 [25600/50000 (51%)]\tLoss: 0.281115\n","[54] Test Loss : 0.3329, Accuracy : 88.83%\n","Train Epoch : 55 [0/50000 (0%)]\tLoss: 0.248894\n","Train Epoch : 55 [25600/50000 (51%)]\tLoss: 0.166579\n","[55] Test Loss : 0.3279, Accuracy : 89.04%\n","Train Epoch : 56 [0/50000 (0%)]\tLoss: 0.292726\n","Train Epoch : 56 [25600/50000 (51%)]\tLoss: 0.252288\n","[56] Test Loss : 0.3280, Accuracy : 89.30%\n","Train Epoch : 57 [0/50000 (0%)]\tLoss: 0.328154\n","Train Epoch : 57 [25600/50000 (51%)]\tLoss: 0.283042\n","[57] Test Loss : 0.3226, Accuracy : 89.47%\n","Train Epoch : 58 [0/50000 (0%)]\tLoss: 0.189070\n"]}],"source":["def train(model, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(DEVICE), target.to(DEVICE)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.cross_entropy(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if(batch_idx % 200 == 0):\n","            print('Train Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.\n","                  format(epoch, batch_idx * len(data),\n","                         len(train_loader.dataset),\n","                         100. * batch_idx / len(train_loader),\n","                         loss.item()))\n","\n","\n","def evaluate(model, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(DEVICE), target.to(DEVICE)\n","            output = model(data)\n","\n","            # 배치 오차 합산\n","            test_loss += F.cross_entropy(output, target,\n","                                    reduction='sum').item()\n","\n","            # 가장 큰 값의 인덱스가 예측값임\n","            pred = output.max(1, keepdim=True)[1]\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","        test_loss /= len(test_loader.dataset)\n","        test_accuracy = 100. * correct / len(test_loader.dataset)\n","        return test_loss, test_accuracy\n","\n","\n","\n","\n","\n","# ResNET, color dataset, residual network\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets\n","\n","USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","\n","\n","\n","# color셋은 흑백보다 복잡하므로 학습을 더 많이해야함.\n","EPOCHS = 300\n","BATCH_SIZE = 128\n","\n","# CIFAR-10 데이터셋 사용 32x32\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.CIFAR10('./.data',\n","                     train=True,\n","                     download=True,\n","                     transform=transforms.Compose([\n","                         transforms.RandomCrop(32, padding=4),\n","                         transforms.RandomHorizontalFlip(),\n","                         transforms.ToTensor(),\n","                         transforms.Normalize((0.5, 0.5, 0.5),\n","                                             (0.5, 0.5, 0.5))])),\n","    batch_size=BATCH_SIZE, shuffle=True )\n","\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.CIFAR10('./.data',\n","                     train=False,\n","                     download=True,\n","                     transform=transforms.Compose([\n","                         transforms.ToTensor(),\n","                         transforms.Normalize((0.5, 0.5, 0.5),\n","                                              (0.5, 0.5, 0.5))])),\n","    batch_size=BATCH_SIZE, shuffle=True )\n","\n","\n","# in_planes: 입력채널수 , planes : 출력 채널수\n","class BasicBlock(nn.Module):\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n","                               stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","\n","        # nn.Sequential()은 여러 모듈을 하나로 묶는 역할\n","        # 이 shortcut은 입력텐서와 출력텐서의 크기를 맞추기 위함.\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, num_classed=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 16\n","\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(16)\n","        self.layer1 = self._make_layer(16, 2, stride=1)\n","        self.layer2 = self._make_layer(32, 2, stride=2)\n","        self.layer3 = self._make_layer(64, 2, stride=2)\n","        self.linear = nn.Linear(64, num_classed)\n","\n","\n","    def _make_layer(self, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","\n","        for stride in strides:\n","            layers.append(BasicBlock(self.in_planes, planes, stride))\n","            self.in_planes = planes\n","\n","        return nn.Sequential(*layers)\n","\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = F.avg_pool2d(out, 8)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","model = ResNet().to(DEVICE)\n","optimizer = optim.SGD(model.parameters(), lr=0.1,\n","                      momentum=0.9, weight_decay=0.0005)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50,\n","                                      gamma=0.1)\n","\n","#print(model) # 모델의 모습 확인하기\n","\n","\n","for epoch in range(1, EPOCHS + 1):\n","    scheduler.step()\n","    train(model, train_loader, optimizer, epoch)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","\n","    print('[{}] Test Loss : {:.4f}, Accuracy : {:.2f}%'.format(\n","        epoch, test_loss, test_accuracy ))\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOT2+s8s6f3s+qblN/ZnP91","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}